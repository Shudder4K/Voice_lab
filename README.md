# GPT-2 TTS: Text-to-Speech Synthesis via GPT-2 + Whisper ASR Evaluation

## Опис

Цей проект демонструє просту, освітню реалізацію моделі Text-to-Speech (TTS) на базі GPT-2, що переводить текст у мел-спектрограму, з подальшою оцінкою якості генерації через автоматичне розпізнавання мовлення (ASR) за допомогою моделі Whisper. Основна мета — зрозуміти процес побудови базового TTS-пайплайну на PyTorch і обчислення Character Error Rate (CER) для реалістичної оцінки результату.

---

## Pipeline

1. **Датасет**: використано справжній датасет LJSpeech (або його фрагмент), що містить пари `аудіо-файл | текст`.
2. **Feature Extraction**: аудіо перетворюється в мел-спектрограми через librosa.
3. **Модель**: простий регресор на базі GPT-2, що прогнозує мел-спектрограму по токенізованому тексту.
4. **Тренування**: мінімізується MSE між згенерованою і таргетною мел-спектрограмою.
5. **Генерація**: тестовий текст → mel → vocoder → аудіо.
6. **Оцінка**: згенероване аудіо проганяється через Whisper ASR, розпізнаний текст порівнюється з target (CER).

---

## Основний код

- **Клас датасету**: вантажить текст і відповідні mel-спектрограми з аудіо.
- **GPT2TTSModel**: розширює GPT-2, додаючи фінальний шар під mel-спектрограму.
- **Train loop**: класичний цикл епох, AdamW, MSELoss.
- **Інференс**: генеруємо мел-спектрограму для нового тексту, переводимо у wav через Griffin-Lim.
- **Оцінка якості**: CER через Whisper ASR.

---

## Приклад результатів у файлі

- **CER (Character Error Rate) тут реальний**: pipeline дійсно конвертує текст → мел → аудіо → текст (через ASR).
- Високий CER означає, що TTS ще не навчився якісно відтворювати мовлення (модель мала малу потужність, просту архітектуру і тренувалась мало епох на урізаному датасеті).
- Графіки мел-спектрограм підтверджують, що mel має структуру, але якість ще низька.

---

## Висновки

- **Pipeline працює повністю (end-to-end):** можна генерувати аудіо і оцінювати CER реалістично.
- **Значення CER** є важливим показником якості TTS, якщо він рахується по ASR на згенерованому аудіо.
- Поточна архітектура GPT2+Linear не підходить для high-quality TTS, але придатна для навчальних експериментів.
- Для справжнього застосування потрібна складніша модель (Tacotron, VITS, FastSpeech), більше даних, більше епох і кращий vocoder.

---

## Вимоги

- `torch`, `transformers`, `librosa`, `soundfile`, `accelerate`, `editdistance`, `matplotlib`
- Pre-trained GPT-2 і Whisper (ASR) через `transformers`
- Датасет LJSpeech (або інший)

---

## Запуск

```bash
# Встановити залежності:
pip install torch transformers librosa soundfile accelerate editdistance matplotlib

# Обов'язково встановити ffmpeg (додати до PATH для роботи аудіо)
# Для Windows: https://ffmpeg.org/download.html




